# -*- coding: utf-8 -*-
import os, csv, time, math, requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# í•­ìƒ py íŒŒì¼ì´ ìˆëŠ” ê²½ë¡œ ê¸°ì¤€ìœ¼ë¡œ ì‹¤í–‰
os.chdir(os.path.dirname(os.path.abspath(__file__)))

API_KEY = "35fc1e9c87f611089e5d742742e4b68a344912a362bc55de129a13717fd49563"

# âš ï¸ SSL ì˜¤ë¥˜ íšŒí”¼: HTTP ë¨¼ì € â†’ ì‹¤íŒ¨ ì‹œ HTTPSë¡œ í´ë°±
BASE_URLS = [
    "http://apis.data.go.kr/1613000/RTMSDataSvcAptTradeDev/getRTMSDataSvcAptTradeDev",
    "https://apis.data.go.kr/1613000/RTMSDataSvcAptTradeDev/getRTMSDataSvcAptTradeDev",
]

# requests ì„¸ì…˜ + ì¬ì‹œë„/ë°±ì˜¤í”„
session = requests.Session()
retry = Retry(
    total=5, connect=5, read=5,
    backoff_factor=1.5,                     # 0,1.5,3,4.5, ...
    status_forcelist=[429,500,502,503,504],
    allowed_methods={"GET"},
    respect_retry_after_header=True,
)
adapter = HTTPAdapter(max_retries=retry, pool_connections=10, pool_maxsize=50)
session.mount("http://", adapter)
session.mount("https://", adapter)
TIMEOUT = (10, 60)                           # (connect, read)
HEADERS = {"Accept": "application/json", "User-Agent": "rtms-collector/2024"}

# ì„œìš¸ ì‹œêµ°êµ¬
SGG_MAP = {
    "11110":"ì¢…ë¡œêµ¬","11140":"ì¤‘êµ¬","11170":"ìš©ì‚°êµ¬","11200":"ì„±ë™êµ¬","11215":"ê´‘ì§„êµ¬",
    "11230":"ë™ëŒ€ë¬¸êµ¬","11260":"ì¤‘ë‘êµ¬","11290":"ì„±ë¶êµ¬","11305":"ê°•ë¶êµ¬","11320":"ë„ë´‰êµ¬",
    "11350":"ë…¸ì›êµ¬","11380":"ì€í‰êµ¬","11410":"ì„œëŒ€ë¬¸êµ¬","11440":"ë§ˆí¬êµ¬","11470":"ì–‘ì²œêµ¬",
    "11500":"ê°•ì„œêµ¬","11530":"êµ¬ë¡œêµ¬","11545":"ê¸ˆì²œêµ¬","11560":"ì˜ë“±í¬êµ¬","11590":"ë™ì‘êµ¬",
    "11620":"ê´€ì•…êµ¬","11650":"ì„œì´ˆêµ¬","11680":"ê°•ë‚¨êµ¬","11710":"ì†¡íŒŒêµ¬","11740":"ê°•ë™êµ¬"
}
SGG_CODES = list(SGG_MAP.keys())

# 2024ë…„ ì „ì²´
MONTHS = [f"2016{str(m).zfill(2)}" for m in range(1, 13)]

def request_json(params):
    """HTTP ìš°ì„  â†’ ì‹¤íŒ¨ ì‹œ HTTPS í´ë°±"""
    last_err = None
    for url in BASE_URLS:
        try:
            r = session.get(url, params=params, headers=HEADERS, timeout=TIMEOUT)
            r.raise_for_status()
            return r.json()
        except (requests.exceptions.SSLError,
                requests.exceptions.ReadTimeout,
                requests.exceptions.ConnectionError) as e:
            last_err = e
            continue
    raise last_err

def fetch_items_all_pages(sgg, ym, num_rows=1000, polite_delay=0.25):
    """ì‹œêµ°êµ¬/ì—°ì›” ì „ í˜ì´ì§€ ìˆ˜ì§‘ (í˜ì´ì§€ë„¤ì´ì…˜ + í´ë°± + ì¬ì‹œë„)"""
    params = {
        "serviceKey": API_KEY,
        "LAWD_CD": sgg,
        "DEAL_YMD": ym,
        "_type": "json",
        "numOfRows": num_rows,
        "pageNo": 1
    }
    try:
        data = request_json(params)
    except Exception as e:
        print(f"âš ï¸ {ym} {sgg} {SGG_MAP.get(sgg,'')}: 1í˜ì´ì§€ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ: {e}")
        return []

    body = (data.get("response") or {}).get("body") or {}
    total = int(body.get("totalCount", 0) or 0)
    if total == 0:
        print(f"â„¹ï¸ {ym} {sgg} {SGG_MAP.get(sgg,'')}: totalCount=0")
        return []

    items = body.get("items", {}).get("item", [])
    if isinstance(items, dict):
        items = [items]
    all_items = list(items)

    pages = math.ceil(total / num_rows)
    for page in range(2, pages + 1):
        time.sleep(polite_delay)
        params["pageNo"] = page
        try:
            data = request_json(params)
            body = (data.get("response") or {}).get("body") or {}
            items = body.get("items", {}).get("item", [])
            if isinstance(items, dict):
                items = [items]
            all_items.extend(items)
        except Exception as e:
            print(f"âš ï¸ {ym} {sgg} p{page} ì‹¤íŒ¨ â†’ ìŠ¤í‚µ: {e}")
            continue

    print(f"âœ… {ym} {sgg} {SGG_MAP.get(sgg,'')}: total={total}, ìˆ˜ì‹ ={len(all_items)}, pages={pages}")
    return all_items

def main():
    rows = []
    for ym in MONTHS:
        for sgg in SGG_CODES:
            items = fetch_items_all_pages(sgg, ym)
            for it in items:
                y = str(it.get("dealYear") or "")
                m = str(it.get("dealMonth") or "").zfill(2) if it.get("dealMonth") is not None else ""
                d = str(it.get("dealDay") or "").zfill(2) if it.get("dealDay") is not None else ""
                deal_date = f"{y}-{m}-{d}" if (y and m and d) else ""

                rows.append([
                    ym, sgg, SGG_MAP.get(sgg, ""),
                    it.get("umdNm"), it.get("aptNm"),
                    it.get("excluUseAr"), it.get("floor"),
                    it.get("dealAmount"),
                    deal_date,
                    it.get("buildYear"), it.get("roadNm"), it.get("jibun"),
                    it.get("dealType"), it.get("estateAgentSggNm"), it.get("buyerGbn")
                ])
            time.sleep(0.5)      # êµ¬ ì‚¬ì´ ê°„ê²©
        time.sleep(1.0)          # ì›” ë‹¨ìœ„ ê°„ê²©

    out = "seoul_real_estate_2016.csv"
    with open(out, "w", newline="", encoding="utf-8-sig") as f:
        w = csv.writer(f)
        w.writerow([
            "ì¡°íšŒì—°ì›”","ì‹œêµ°êµ¬ì½”ë“œ","ì‹œêµ°êµ¬ëª…","ë²•ì •ë™","ì•„íŒŒíŠ¸ëª…","ì „ìš©ë©´ì (ã¡)",
            "ì¸µ","ê±°ë˜ê¸ˆì•¡(ë§Œì›)","ê±°ë˜ì¼ì","ê±´ì¶•ë…„ë„","ë„ë¡œëª…","ì§€ë²ˆ",
            "ê±°ë˜ìœ í˜•","ì¤‘ê°œì‚¬ë¬´ì†Œëª…","ê±°ë˜ìêµ¬ë¶„"
        ])
        w.writerows(rows)

    print(f"\nğŸ“ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {out} (ì´ {len(rows)}ê±´)")

if __name__ == "__main__":
    main()